- job-template:
    name: 'kubernetes-e2e-{gke-suffix}'
    node: '{jenkins_node}'
    description: '{description} Test owner: {test-owner}.'
    jenkins_node: 'e2e'
    disabled: '{obj:disable_job}'
    properties:
        - build-discarder:
            days-to-keep: 7
    # Need the 8 essential kube-system pods ready before declaring cluster ready
    # etcd-server, kube-apiserver, kube-controller-manager, kube-dns
    # kube-scheduler, l7-default-backend, l7-lb-controller, kube-addon-manager
    provider-env: |
        export CLOUDSDK_API_ENDPOINT_OVERRIDES_CONTAINER="https://test-container.sandbox.googleapis.com/"
        export CLOUDSDK_BUCKET="gs://cloud-sdk-testing/ci/staging"
        export E2E_MIN_STARTUP_PODS="8"
        export FAIL_ON_GCP_RESOURCE_LEAK="true"
        export KUBERNETES_PROVIDER="gke"
        export ZONE="us-central1-f"
    builders:
        - activate-gce-service-account
        - shell: |
            {provider-env}
            {job-env}
            {post-env}
            timeout -k {kill-timeout}m {timeout}m {runner} && rc=$? || rc=$?
            {report-rc}
    wrappers:
        - e2e-credentials-binding
        - timeout:
            timeout: '{jenkins-timeout}'
            fail: true
        - workspace-cleanup:
            dirmatch: true
            external-deletion-command: 'sudo rm -rf %s'
            exclude: ['**/.git/']
    triggers:
        - reverse:
            jobs: '{trigger-job}'
            result: success
        - timed: '{cron-string}'
    publishers:
        - junit-publisher
        - email-ext:
            recipients: '{emails}'
        - gcs-uploader

# Manual jobs, migrate later.
- project:
    name: kubernetes-e2e-gke-master
    trigger-job: 'kubernetes-build'
    test-owner: 'Build Cop'
    gke-suffix:
        - 'gke-large-cluster':  # kubernetes-e2e-gke-large-cluster
            description: 'Run all non-flaky, non-slow, non-disruptive, non-feature tests on GKE, in parallel on a large GKE cluster'
            timeout: 600
            emails: 'zml@google.com wojtekt@google.com'
            # TODO: Enable again when we're done with 3000-ndoe testing.
            # cron-string: '0 17 * * *'
            cron-string: ''
            trigger-job: ''
            job-env: |
                export E2E_NAME="gke-large-cluster"
                export PROJECT="kubernetes-scale"
                # TODO: Remove FAIL_ON_GCP_RESOURCE_LEAK when PROJECT changes back to gke-large-cluster-jenkins.
                export FAIL_ON_GCP_RESOURCE_LEAK="false"
                # TODO: should test kube-proxy test is not designed to run in large clusters.
                #   We should change it start running it here too.
                export GINKGO_TEST_ARGS="--ginkgo.skip=\[Serial\]|\[Disruptive\]|\[Flaky\]|\[Feature:.+\]|should\stest\skube-proxy \
                                         --allowed-not-ready-nodes=30 \
                                         --system-pods-startup-timeout=300m"
                export GINKGO_PARALLEL="y"
                export ZONE="us-east1-a"
                export NUM_NODES=3000
                export MACHINE_TYPE="n1-standard-1"
                export HEAPSTER_MACHINE_TYPE="n1-standard-8"
                export ALLOWED_NOTREADY_NODES="30"
                # We were asked (by MIG team) to not create more than 5 MIGs per zone.
                # We also paged SREs with max-nodes-per-pool=400 (5 concurrent MIGs)
                # So setting max-nodes-per-pool=1000, to check if that helps.
                export GKE_CREATE_FLAGS="--max-nodes-per-pool=1000"
                export CLOUDSDK_CONTAINER_USE_CLIENT_CERTIFICATE=True
                export CLOUDSDK_API_ENDPOINT_OVERRIDES_CONTAINER="https://staging-container.sandbox.googleapis.com/"
                export KUBE_GKE_IMAGE_TYPE="gci"
        - 'gke-large-deploy':  # kubernetes-e2e-gke-large-deploy
            description: 'Starts up a large GKE cluster for further manual testing.'
            timeout: 300
            emails: 'gmarek@google.com wojtekt@google.com'
            cron-string: ''
            trigger-job: ''
            job-env: |
                export E2E_NAME="gke-large-deploy"
                export PROJECT="kubernetes-scale"
                # TODO: Remove FAIL_ON_GCP_RESOURCE_LEAK when PROJECT changes back to gke-large-cluster-jenkins.
                export FAIL_ON_GCP_RESOURCE_LEAK="false"
                export GINKGO_TEST_ARGS="--ginkgo.focus=\[Feature:Empty\] \
                                         --allowed-not-ready-nodes=20 \
                                         --system-pods-startup-timeout=300m"
                export ZONE="us-east1-a"
                export NUM_NODES=3000
                export MACHINE_TYPE="n1-standard-1"
                export HEAPSTER_MACHINE_TYPE="n1-standard-8"
                export ALLOWED_NOTREADY_NODES="20"
                # We were asked (by MIG team) to not create more than 5 MIGs per zone.
                # We also paged SREs with max-nodes-per-pool=400 (5 concurrent MIGs)
                # So setting max-nodes-per-pool=1000, to check if that helps.
                export GKE_CREATE_FLAGS="--max-nodes-per-pool=1000"
                export CLOUDSDK_CONTAINER_USE_CLIENT_CERTIFICATE=True
                export CLOUDSDK_API_ENDPOINT_OVERRIDES_CONTAINER="https://staging-container.sandbox.googleapis.com/"
                export KUBE_NODE_OS_DISTRIBUTION="debian"
                export E2E_DOWN="false"
        - 'gke-large-teardown':  # kubernetes-e2e-gke-large-deploy
            description: 'Tears down cluster created by gke-large-deploy.'
            timeout: 180
            emails: 'gmarek@google.com wojtekt@google.com'
            cron-string: ''
            trigger-job: ''
            job-env: |
                export E2E_NAME="gke-large-deploy"
                export PROJECT="kubernetes-scale"
                # TODO: Remove FAIL_ON_GCP_RESOURCE_LEAK when PROJECT changes back to gke-large-cluster-jenkins.
                export FAIL_ON_GCP_RESOURCE_LEAK="false"
                export ZONE="us-east1-a"
                export NUM_NODES=2000
                export MACHINE_TYPE="n1-standard-1"
                export HEAPSTER_MACHINE_TYPE="n1-standard-4"
                export ALLOWED_NOTREADY_NODES="20"
                # We were asked (by MIG team) to not create more than 5 MIGs per zone.
                # We also paged SREs with max-nodes-per-pool=400 (5 concurrent MIGs)
                # So setting max-nodes-per-pool=1000, to check if that helps.
                export GKE_CREATE_FLAGS="--max-nodes-per-pool=1000"
                export CLOUDSDK_CONTAINER_USE_CLIENT_CERTIFICATE=True
                export CLOUDSDK_API_ENDPOINT_OVERRIDES_CONTAINER="https://staging-container.sandbox.googleapis.com/"
                export KUBE_NODE_OS_DISTRIBUTION="debian"
                export E2E_TEST="false"
                export E2E_UP="false"
    jobs:
        - 'kubernetes-e2e-{gke-suffix}'
